{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T11:54:05.359354Z",
     "iopub.status.busy": "2024-04-06T11:54:05.358871Z",
     "iopub.status.idle": "2024-04-06T11:54:23.823325Z",
     "shell.execute_reply": "2024-04-06T11:54:23.822129Z",
     "shell.execute_reply.started": "2024-04-06T11:54:05.359320Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "import emoji\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random_seed = 2024\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T11:54:23.826173Z",
     "iopub.status.busy": "2024-04-06T11:54:23.825444Z",
     "iopub.status.idle": "2024-04-06T11:54:23.837113Z",
     "shell.execute_reply": "2024-04-06T11:54:23.835623Z",
     "shell.execute_reply.started": "2024-04-06T11:54:23.826137Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define configuration\n",
    "class Config():\n",
    "    seed = random_seed\n",
    "    word_count = 1000\n",
    "    train_path = \"/kaggle/input/nlp-getting-started/train.csv\"\n",
    "    test_path = \"/kaggle/input/nlp-getting-started/test.csv\"\n",
    "\n",
    "# Function to remove HTML tags\n",
    "def remove_html_tags(text):\n",
    "    html_pattern = re.compile(r'<.*?>')\n",
    "    return html_pattern.sub(r'', text)\n",
    "\n",
    "# Function for data preprocessing\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = text.lower()\n",
    "    text = remove_html_tags(text)\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "    text = re.sub(\"@\\w+\", '', text)\n",
    "    text = re.sub(\"'\\d+\", '', text)\n",
    "    text = re.sub(\"\\d+\", '', text)\n",
    "    text = re.sub(r\"[^\\w\\s]\", '', text)\n",
    "    text = re.sub(\"http\\w+\", '', text)\n",
    "    text = re.sub(\"\\s[a-z]\\s\", '', text)\n",
    "    text = text.strip()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T11:54:23.839785Z",
     "iopub.status.busy": "2024-04-06T11:54:23.839002Z",
     "iopub.status.idle": "2024-04-06T11:54:31.647431Z",
     "shell.execute_reply": "2024-04-06T11:54:31.646138Z",
     "shell.execute_reply.started": "2024-04-06T11:54:23.839741Z"
    }
   },
   "outputs": [],
   "source": [
    "# Seed everything\n",
    "np.random.seed(Config.seed)\n",
    "random.seed(Config.seed)\n",
    "tf.random.set_seed(Config.seed)\n",
    "\n",
    "# Read data\n",
    "train_df = pd.read_csv(Config.train_path)\n",
    "train_df['text'] = train_df['text'].apply(preprocess_text)\n",
    "print(f\"len(train_df): {len(train_df)}\")\n",
    "\n",
    "test_df = pd.read_csv(Config.test_path)\n",
    "test_df['text'] = test_df['text'].apply(preprocess_text)\n",
    "print(f\"len(test_df): {len(test_df)}\")\n",
    "\n",
    "# Create word dictionary\n",
    "text = train_df['text'].values\n",
    "word_dict = {}\n",
    "for i in range(len(text)):\n",
    "    for j in range(len(text[i])):\n",
    "        if text[i][j] in word_dict:\n",
    "            word_dict[text[i][j]] += 1\n",
    "        else:\n",
    "            word_dict[text[i][j]] = 1\n",
    "\n",
    "sorted_items = sorted(word_dict.items(), key=lambda x: -x[1])\n",
    "sorted_list = list(sorted_items)\n",
    "top_words = [value[0] for value in sorted_list[:Config.word_count]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T11:54:31.650837Z",
     "iopub.status.busy": "2024-04-06T11:54:31.650079Z",
     "iopub.status.idle": "2024-04-06T11:54:35.856738Z",
     "shell.execute_reply": "2024-04-06T11:54:35.855393Z",
     "shell.execute_reply.started": "2024-04-06T11:54:31.650787Z"
    }
   },
   "outputs": [],
   "source": [
    "train_text=train_df['text'].values\n",
    "test_text=test_df['text'].values\n",
    "X=np.array([[int(top_word in text) for top_word in top_words] for text in train_text])\n",
    "y=train_df['target'].values\n",
    "test_X=np.array([[int(top_word in text) for top_word in top_words] for text in test_text])\n",
    "print(f\"X.shape:{X.shape},y.shape:{y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T11:54:35.858632Z",
     "iopub.status.busy": "2024-04-06T11:54:35.858203Z",
     "iopub.status.idle": "2024-04-06T11:54:35.890747Z",
     "shell.execute_reply": "2024-04-06T11:54:35.889601Z",
     "shell.execute_reply.started": "2024-04-06T11:54:35.858594Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T11:54:35.892994Z",
     "iopub.status.busy": "2024-04-06T11:54:35.892553Z",
     "iopub.status.idle": "2024-04-06T11:54:35.916247Z",
     "shell.execute_reply": "2024-04-06T11:54:35.914916Z",
     "shell.execute_reply.started": "2024-04-06T11:54:35.892957Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df[train_df[\"target\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T11:54:35.919037Z",
     "iopub.status.busy": "2024-04-06T11:54:35.918473Z",
     "iopub.status.idle": "2024-04-06T11:54:35.935471Z",
     "shell.execute_reply": "2024-04-06T11:54:35.933849Z",
     "shell.execute_reply.started": "2024-04-06T11:54:35.918992Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df[train_df[\"target\"] == 0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T11:54:35.938267Z",
     "iopub.status.busy": "2024-04-06T11:54:35.937592Z",
     "iopub.status.idle": "2024-04-06T11:54:36.610549Z",
     "shell.execute_reply": "2024-04-06T11:54:36.609219Z",
     "shell.execute_reply.started": "2024-04-06T11:54:35.938233Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Visualizing the target classes\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.title(\"Count of Target Classes\")\n",
    "sns.countplot(y=train_df[\"target\"],linewidth=2,\n",
    "                   edgecolor='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start by analysing total number of characters in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T11:54:36.613388Z",
     "iopub.status.busy": "2024-04-06T11:54:36.612119Z",
     "iopub.status.idle": "2024-04-06T11:54:37.315676Z",
     "shell.execute_reply": "2024-04-06T11:54:37.314721Z",
     "shell.execute_reply.started": "2024-04-06T11:54:36.613353Z"
    }
   },
   "outputs": [],
   "source": [
    "fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\n",
    "char_len_dis = train_df[train_df['target']==1]['text'].str.len()\n",
    "ax1.hist(char_len_dis,color='red',edgecolor='black', linewidth=1.2)\n",
    "ax1.set_title('Disaster Tweets')\n",
    "char_len_ndis = train_df[train_df['target']==0]['text'].str.len()\n",
    "ax2.hist(char_len_ndis,color='blue',edgecolor='black', linewidth=1.2)\n",
    "ax2.set_title('Non-Disaster Tweets')\n",
    "plt.suptitle(\"Length of Characters in text\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T11:54:37.319489Z",
     "iopub.status.busy": "2024-04-06T11:54:37.318853Z",
     "iopub.status.idle": "2024-04-06T11:54:37.324716Z",
     "shell.execute_reply": "2024-04-06T11:54:37.323822Z",
     "shell.execute_reply.started": "2024-04-06T11:54:37.319457Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating sample corpus for further analysis.\n",
    "def create_corpus(target):\n",
    "    corpus = []\n",
    "    for x in train_df[train_df['target']==target]['text'].str.split():\n",
    "        for i in x:\n",
    "            corpus.append(i)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T11:54:37.326699Z",
     "iopub.status.busy": "2024-04-06T11:54:37.325889Z",
     "iopub.status.idle": "2024-04-06T11:54:38.205942Z",
     "shell.execute_reply": "2024-04-06T11:54:38.204825Z",
     "shell.execute_reply.started": "2024-04-06T11:54:37.326654Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to create corpus\n",
    "def create_corpus(text):\n",
    "    corpus = []\n",
    "    for sentence in text:\n",
    "        for word in sentence:\n",
    "            corpus.append(word)\n",
    "    return corpus\n",
    "\n",
    "# Function to analyze top stop words in text\n",
    "def analyze_stopwords(data, func, target):\n",
    "    values_list = []\n",
    "    for label in target:\n",
    "        dic = defaultdict(int)\n",
    "        corpus = func(data[data['target'] == label]['text'])\n",
    "        for word in corpus:\n",
    "            dic[word] += 1\n",
    "        top = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        x_items, y_values = zip(*top)\n",
    "        values_list.extend(list(x_items))\n",
    "        values_list.extend(list(y_values))\n",
    "        \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    ax1.barh(values_list[:10], values_list[10:20], color=\"lightblue\", edgecolor='black', linewidth=1.2)\n",
    "    ax1.set_title(\"Non-Disaster Tweets\")\n",
    "    \n",
    "    ax2.barh(values_list[20:30], values_list[30:], color=\"lightgreen\", edgecolor='black', linewidth=1.2)\n",
    "    ax2.set_title(\"Disaster Tweets\")\n",
    "            \n",
    "    plt.suptitle(\"Top Stop words in text\")\n",
    "    plt.show()\n",
    "\n",
    "# Call analyze_stopwords function\n",
    "analyze_stopwords(train_df, create_corpus, [0, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T11:54:38.207781Z",
     "iopub.status.busy": "2024-04-06T11:54:38.207230Z",
     "iopub.status.idle": "2024-04-06T11:54:38.707899Z",
     "shell.execute_reply": "2024-04-06T11:54:38.706682Z",
     "shell.execute_reply.started": "2024-04-06T11:54:38.207750Z"
    }
   },
   "outputs": [],
   "source": [
    "# Analysing Top 20  disastrous KeyWords in text .\n",
    "plt.figure(figsize=(10,7))\n",
    "train_df[train_df['target']==1]['keyword'].value_counts()[:20].plot(kind='barh', fontsize=12,title='Top 20 Disastrous Keywords in Text', color='#0096FF',edgecolor='black', linewidth=1.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T11:54:38.710512Z",
     "iopub.status.busy": "2024-04-06T11:54:38.709736Z",
     "iopub.status.idle": "2024-04-06T11:54:39.173562Z",
     "shell.execute_reply": "2024-04-06T11:54:39.172597Z",
     "shell.execute_reply.started": "2024-04-06T11:54:38.710449Z"
    }
   },
   "outputs": [],
   "source": [
    "# Analysing Top 20 disastrous Locations in text.\n",
    "plt.figure(figsize=(10,7))\n",
    "train_df[train_df[\"target\"]==1][\"location\"].value_counts()[:20].plot(kind='barh',fontsize=12, title='Top 20 Disastrous Locations in Text', color='#66ff00',edgecolor='black', linewidth=1.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T11:57:23.140826Z",
     "iopub.status.busy": "2024-04-06T11:57:23.140338Z",
     "iopub.status.idle": "2024-04-06T11:58:06.312861Z",
     "shell.execute_reply": "2024-04-06T11:58:06.311551Z",
     "shell.execute_reply.started": "2024-04-06T11:57:23.140793Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Flatten\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_df['text'], train_df['target'], test_size=0.2, random_state=2024)\n",
    "\n",
    "# Convert text to sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_words = 1000\n",
    "max_len = 100\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "# Pad sequences\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "X_val_pad = pad_sequences(X_val_seq, maxlen=max_len)\n",
    "\n",
    "# Build CNN model\n",
    "embedding_dim = 100\n",
    "vocab_size = max_words\n",
    "\n",
    "cnn_model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim),\n",
    "    Conv1D(128, 5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train CNN model\n",
    "history = cnn_model.fit(X_train_pad, y_train,\n",
    "                        epochs=10,\n",
    "                        batch_size=64,\n",
    "                        validation_data=(X_val_pad, y_val))\n",
    "\n",
    "# Evaluate CNN model on validation set\n",
    "val_loss, val_accuracy = cnn_model.evaluate(X_val_pad, y_val)\n",
    "print(f'Validation accuracy: {val_accuracy}')\n",
    "\n",
    "# Predict on test set\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_df['text'])\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
    "test_pred = (cnn_model.predict(X_test_pad) > 0.5).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T11:58:07.932935Z",
     "iopub.status.busy": "2024-04-06T11:58:07.932390Z",
     "iopub.status.idle": "2024-04-06T11:58:07.949379Z",
     "shell.execute_reply": "2024-04-06T11:58:07.947371Z",
     "shell.execute_reply.started": "2024-04-06T11:58:07.932895Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load true labels of the test data\n",
    "true_labels = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")['target']\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = accuracy_score(true_labels, test_pred)\n",
    "print(f'Test accuracy: {test_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T11:58:09.819167Z",
     "iopub.status.busy": "2024-04-06T11:58:09.818717Z",
     "iopub.status.idle": "2024-04-06T11:58:11.224545Z",
     "shell.execute_reply": "2024-04-06T11:58:11.222789Z",
     "shell.execute_reply.started": "2024-04-06T11:58:09.819135Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Obtain predictions from the CNN model on the test set\n",
    "y_pred_proba = cnn_model.predict(X_test_pad)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, _ = roc_curve(true_labels, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g', cbar=False)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 869809,
     "sourceId": 17777,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
